{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T01:26:27.425157Z",
     "start_time": "2024-11-14T01:26:27.386150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from benchmark_visualizer import BenchmarkVisualizer\n",
    "\n",
    "visualizer = BenchmarkVisualizer([\n",
    "    \"results/bplusarray_nocache_results.csv\",\n",
    "    \"results/hashmap_default_results.csv\",\n",
    "    \"results/skiphash_default_results.csv\",\n",
    "    \"results/skipindexed_default_results.csv\",\n",
    "])"
   ],
   "id": "8bcce11301bfab13",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error processing results/bplusarray_nocache_results.csv: Missing required columns in results/bplusarray_nocache_results.csv: AverageExecutionTime, HeapMemoryUsed, HeapMemoryDelta",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mE:\\.current\\projects\\CS201-proj-SkipList\\SmuSQL\\src\\main\\java\\edu\\smu\\smusql\\analysis\\visualization\\benchmark_visualizer.py:57\u001B[0m, in \u001B[0;36mBenchmarkVisualizer.__init__\u001B[1;34m(self, csv_files)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m missing_columns:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     58\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required columns in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(missing_columns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     59\u001B[0m     )\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Add engine and configuration information\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Missing required columns in results/bplusarray_nocache_results.csv: AverageExecutionTime, HeapMemoryUsed, HeapMemoryDelta",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbenchmark_visualizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BenchmarkVisualizer\n\u001B[1;32m----> 3\u001B[0m visualizer \u001B[38;5;241m=\u001B[39m \u001B[43mBenchmarkVisualizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresults/bplusarray_nocache_results.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresults/hashmap_default_results.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresults/skiphash_default_results.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresults/skipindexed_default_results.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\.current\\projects\\CS201-proj-SkipList\\SmuSQL\\src\\main\\java\\edu\\smu\\smusql\\analysis\\visualization\\benchmark_visualizer.py:73\u001B[0m, in \u001B[0;36mBenchmarkVisualizer.__init__\u001B[1;34m(self, csv_files)\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCSV file is empty: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 73\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError processing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# Combine all DataFrames\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mValueError\u001B[0m: Error processing results/bplusarray_nocache_results.csv: Missing required columns in results/bplusarray_nocache_results.csv: AverageExecutionTime, HeapMemoryUsed, HeapMemoryDelta"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T01:26:27.427157Z",
     "start_time": "2024-11-14T01:26:27.426157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Execution Time Distribution Analysis\")\n",
    "visualizer.plot_execution_time_distribution()"
   ],
   "id": "66dbadbff0423ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- DELETE and single-point UPDATE operations are consistently fast across all data structures.\n",
    "- RANGE SELECT and RANGE UPDATE are more time-intensive, with RANGE UPDATE showing the highest computational demands, potentially favoring `bplusarray_withcache` for ordered operations.\n",
    "- INSERT operations are consistent across all implementations except for `bplushashmap_withcache`, which shows a significant increase in execution time. \n",
    "- SIMPLE SELECT show a larger variability across implementations with the cached implementations showing a slight advantage.\n"
   ],
   "id": "2b05152cb69ebe54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Query Time Trend Analysis\")\n",
    "visualizer.plot_query_time_trends()"
   ],
   "id": "c91752a51df83fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cache Impact\n",
    "- Cache implementations generally perform similarly to their \"nocache\" counterparts in most operations\n",
    "- In SIMPLE_SELECT operations, having cache appears to provide better performance for the array implementation\n",
    "\n",
    "## Data Structure Comparison\n",
    "- Hashmap tends to have slightly higher execution times compared to bplusarray in most operations"
   ],
   "id": "91973235eca3e33c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Range vs Equals Operations Analysis\")\n",
    "visualizer.plot_range_vs_equals_comparison()"
   ],
   "id": "15d1915bb161a075",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- RANGE operations are generally faster than EQUAL operations\n",
    "- `bplusarray` shows slightly better performance characteristics overall\n",
    "- Addition of cache doesn't significantly improve performance in most cases"
   ],
   "id": "d3068b08b7cd4964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Performance Comparison Dashboard\")\n",
    "visualizer.plot_performance_comparison()"
   ],
   "id": "54390bd91635971b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Overall based on execution time and memory usage, `bplusarray` with cache appears to be the best performing data structure",
   "id": "2b64995e34217179"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualizer = BenchmarkVisualizer([\n",
    "    \"results/hashmap_default_results.csv\",\n",
    "    \"results/skiphash_default_results.csv\",\n",
    "    \"results/skipindexed_default_results.csv\",\n",
    "])"
   ],
   "id": "a250d1ced4fb6209",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
